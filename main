import argparse
from preprocess_doc import embed_doc
from models import load_retrieval_model, load_inference_model
from inference import answer_question, get_relevant_chunks



if __name__ == "__main__":
    # Load the retrieval model
    retrieval_model = load_retrieval_model()
    # Load the inference model
    inference_tokenizer, inference_model = load_inference_model()

    args = argparse.ArgumentParser()
    args.add_argument("--doc_path", type=str, default="data/India_capital.docx")
    args = args.parse_args()

    # Load the document
    chunk_embeddings, chunks = embed_doc(args.doc_path, retrieval_model)

    # After question is asked   
    while True:
        question = input("Ask a question: ")
        if question == "exit":
            break
        # Retrieve the most relevant chunks
        retrieved_chunks = get_relevant_chunks(retrieval_model, chunk_embeddings, chunks, question)
        # Answer the question
        answer = answer_question(inference_tokenizer, inference_model, retrieved_chunks, question)
        print("Answer:", answer)
